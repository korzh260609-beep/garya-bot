// src/bot/handlers/chat.js
// extracted from messageRouter.js â€” no logic changes (only safety-guards + token param fix + observability logs)
//
// STAGE 7.2 LOGIC: pass globalUserId to chat_memory (v2 columns)

import pool from "../../../db.js";
import { getMemoryService } from "../../core/memoryServiceFactory.js";
import { getRecallEngine } from "../../core/recallEngineFactory.js";
import { getAlreadySeenDetector } from "../../core/alreadySeenFactory.js";
import { createTimeContext } from "../../core/time/timeContextFactory.js";
import { touchChatMeta } from "../../db/chatMeta.js";
import { redactText, sha256Text, buildRawMeta } from "../../core/redaction.js";

export async function handleChatMessage({
  bot,
  msg,
  chatId,
  chatIdStr,
  senderIdStr,
  trimmed,
  bypass,
  MAX_HISTORY_MESSAGES = 20,

  //  STAGE 7.2
  globalUserId = null,

  FileIntake,

  saveMessageToMemory,
  getChatHistory,
  saveChatPair,

  logInteraction,

  loadProjectContext,
  getAnswerMode,
  buildSystemPrompt,
  isMonarch,

  callAI,
  sanitizeNonMonarchReply,
}) {
  const messageId = msg.message_id ?? null;
  if (!trimmed) return;

  // STAGE 7B.5.x â€” Free-tier DB growth protection (hard cap)
  const MAX_CHAT_MESSAGE_CHARS = 16000;

  // ---- GUARDS (critical): never crash on wrong wiring ----
  const isMonarchFn = typeof isMonarch === "function" ? isMonarch : () => false;
  const monarchNow = isMonarchFn(senderIdStr);

  if (typeof callAI !== "function") {
    const details =
      "callAI is not a function (router wiring error: pass { callAI } into handleChatMessage).";
    let text = "ERROR: ÐžÑˆÐ¸Ð±ÐºÐ° Ð²Ñ‹Ð·Ð¾Ð²Ð° Ð˜Ð˜.";
    if (monarchNow) {
      text = "ERROR: ÐžÑˆÐ¸Ð±ÐºÐ° ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ð¸: " + details;
    }

    try {
      await bot.sendMessage(chatId, text);
    } catch (e) {
      console.error("ERROR Telegram send error (callAI guard):", e);
    }
    return;
  }
  // --------------------------------------------

  const summarizeMediaAttachment =
    typeof FileIntake?.summarizeMediaAttachment === "function"
      ? FileIntake.summarizeMediaAttachment
      : () => null;

  const mediaSummary = summarizeMediaAttachment(msg);

  const decisionFn =
    typeof FileIntake?.buildEffectiveUserTextAndDecision === "function"
      ? FileIntake.buildEffectiveUserTextAndDecision
      : null;

  const decision = decisionFn
    ? decisionFn(trimmed, mediaSummary)
    : {
        effectiveUserText: trimmed,
        shouldCallAI: Boolean(trimmed),
        directReplyText: Boolean(trimmed) ? null : "ÐÐ°Ð¿Ð¸ÑˆÐ¸ Ñ‚ÐµÐºÑÑ‚Ð¾Ð¼, Ñ‡Ñ‚Ð¾ Ð½ÑƒÐ¶Ð½Ð¾ ÑÐ´ÐµÐ»Ð°Ñ‚ÑŒ.",
      };

  const effective = (decision?.effectiveUserText || "").trim();
  const shouldCallAI = Boolean(decision?.shouldCallAI);
  const directReplyText = decision?.directReplyText || null;

  // ==========================================================
  // STAGE 7B.10 + 7B.5.3 â€” Redaction + hard cap for chat_messages storage
  // Store ONLY redacted content in chat_messages.
  // ==========================================================
  const userRedactedFull = redactText(effective);

  const userContentForDb =
    typeof userRedactedFull === "string" && userRedactedFull.length > MAX_CHAT_MESSAGE_CHARS
      ? userRedactedFull.slice(0, MAX_CHAT_MESSAGE_CHARS)
      : userRedactedFull;

  const userTruncatedForDb =
    typeof userRedactedFull === "string" && userRedactedFull.length > MAX_CHAT_MESSAGE_CHARS;

  const userTextHash = sha256Text(userRedactedFull);

  if (directReplyText) {
    try {
      await bot.sendMessage(chatId, directReplyText);
    } catch (e) {
      console.error("ERROR Telegram send error (directReplyText):", e);
    }
    return;
  }

  if (!shouldCallAI) {
    try {
      await bot.sendMessage(chatId, "ÐÐ°Ð¿Ð¸ÑˆÐ¸ Ñ‚ÐµÐºÑÑ‚Ð¾Ð¼, Ñ‡Ñ‚Ð¾ Ð½ÑƒÐ¶Ð½Ð¾ ÑÐ´ÐµÐ»Ð°Ñ‚ÑŒ.");
    } catch (e) {
      console.error("ERROR Telegram send error (shouldCallAI):", e);
    }
    return;
  }

  // ==========================================================
  // STAGE 7B.7 â€” IDEMPOTENCY CORE (critical)
  // Insert-first into chat_messages to guarantee process-once on webhook retries.
  //
  // Strategy:
  // - Only for inbound USER messages with numeric Telegram message_id
  // - INSERT ... ON CONFLICT DO NOTHING (partial unique index for role='user')
  // - If conflict => already processed => exit WITHOUT calling AI and WITHOUT sending a second reply
  // - Fail-open on any DB error (do not break production)
  //
  // STAGE 7B.5.2 â€” raw meta only (no text/binary)
  // STAGE 7B.2 â€” platform_message_id + text_hash
  // STAGE 7B.10 â€” content redacted
  // ==========================================================
  if (messageId !== null && Number.isFinite(Number(messageId))) {
    try {
      const transport = "telegram";
      const chatType = msg?.chat?.type || null;
      const senderId = senderIdStr || null;

      const meta = {
        senderIdStr,
        chatIdStr,
        messageId,
        globalUserId,
      };

      //  7B.5.2: meta-only raw (NO msg text, NO attachments)
      const raw = buildRawMeta(msg);

      const ins = await pool.query(
        `
        INSERT INTO chat_messages (
          transport,
          chat_id,
          chat_type,
          global_user_id,
          sender_id,
          message_id,
          platform_message_id,
          text_hash,
          role,
          content,
          truncated,
          metadata,
          raw,
          schema_version
        )
        VALUES ($1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12::jsonb,$13::jsonb,$14)
        ON CONFLICT (transport, chat_id, message_id)
          WHERE role='user' AND message_id IS NOT NULL
        DO NOTHING
        RETURNING id
        `,
        [
          transport,
          String(chatIdStr),
          chatType ? String(chatType) : null,
          globalUserId ? String(globalUserId) : null,
          senderId ? String(senderId) : null,
          Number(messageId),
          Number(messageId), // platform_message_id (telegram = message_id)
          userTextHash,
          "user",
          userContentForDb,
          Boolean(userTruncatedForDb),
          JSON.stringify(meta),
          JSON.stringify(raw),
          1,
        ]
      );

      // Conflict => already processed (retry) => exit silently
      if (!ins || (ins.rowCount || 0) === 0) {
        try {
          console.info("IDEMPOTENCY_SKIP", {
            transport,
            chatId: chatIdStr,
            messageId,
            reason: "chat_messages_conflict",
          });
        } catch (_) {}

        //  STAGE 7B.7 OBSERVABILITY: persist dedupe-hit (legacy attempt; may be ignored if schema doesn't support it)
        try {
          await logInteraction(chatIdStr, {
            taskType: "chat",
            aiCostLevel: "none",
            event: "WEBHOOK_DEDUPE_HIT",
          });
        } catch (e) {
          console.error("ERROR logInteraction (WEBHOOK_DEDUPE_HIT) error:", e);
        }

        //  STAGE 7B.7 OBSERVABILITY (V2): dedicated table webhook_dedupe_events
        try {
          await pool.query(
            `
            INSERT INTO webhook_dedupe_events
            (transport, chat_id, message_id, global_user_id, reason, metadata)
            VALUES ($1,$2,$3,$4,$5,$6::jsonb)
            ON CONFLICT (transport, chat_id, message_id) DO NOTHING
            `,
            [
              transport,
              String(chatIdStr),
              Number(messageId),
              globalUserId ? String(globalUserId) : null,
              "retry_duplicate",
              JSON.stringify({ handler: "chat", stage: "7B.7" }),
            ]
          );
        } catch (e) {
          console.error("ERROR webhook_dedupe_events insert failed:", e);
        }

        return;
      }

      // STAGE 7B.8 â€” touch chat_meta (inbound user)
      try {
        await touchChatMeta({
          transport,
          chatId: String(chatIdStr),
          chatType: msg?.chat?.type || null,
          title: msg?.chat?.title || null,
          role: "user",
        });
      } catch (_) {}
    } catch (e) {
      console.error("ERROR STAGE 7B.7 chat_messages insert-first failed (fail-open):", e);
      // fail-open: continue normal flow
    }
  }

  //  STAGE 7.2: save with globalUserId + metadata
  // NOTE: Memory layer keeps original text; 7B redaction applies to chat_history (chat_messages) only.
  try {
    await saveMessageToMemory(chatIdStr, "user", effective, {
      globalUserId,
      transport: "telegram",
      metadata: { senderIdStr, chatIdStr, messageId },
      schemaVersion: 2,
    });
  } catch (e) {
    console.error("ERROR saveMessageToMemory error:", e);
  }

  let history = [];
  try {
    //  STAGE 7.3: read history via MemoryService (ban direct/legacy SQL reads here)
    const memory = getMemoryService();
    history = await memory.recent({
      chatId: chatIdStr,
      globalUserId,
      limit: MAX_HISTORY_MESSAGES,
    });
  } catch (e) {
    console.error("ERROR memory.recent error:", e);
  }

  const classification = { taskType: "chat", aiCostLevel: "low" };
  try {
    await logInteraction(chatIdStr, classification);
  } catch (e) {
    console.error("ERROR logInteraction error:", e);
  }

  let projectCtx = "";
  try {
    projectCtx = await loadProjectContext();
  } catch (e) {
    console.error("ERROR loadProjectContext error:", e);
  }

  const answerMode = getAnswerMode(chatIdStr);

  let modeInstruction = "";
  if (answerMode === "short") {
    modeInstruction =
      "Ð ÐµÐ¶Ð¸Ð¼ short: Ð¾Ñ‚Ð²ÐµÑ‡Ð°Ð¹ Ð¾Ñ‡ÐµÐ½ÑŒ ÐºÑ€Ð°Ñ‚ÐºÐ¾ (1â€“2 Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ), Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¿Ð¾ ÑÑƒÑ‰ÐµÑÑ‚Ð²Ñƒ, Ð±ÐµÐ· Ð»Ð¸ÑˆÐ½Ð¸Ñ… Ð´ÐµÑ‚Ð°Ð»ÐµÐ¹.";
  } else if (answerMode === "normal") {
    modeInstruction =
      "Ð ÐµÐ¶Ð¸Ð¼ normal: Ð´Ð°Ð²Ð°Ð¹ Ñ€Ð°Ð·Ð²Ñ‘Ñ€Ð½ÑƒÑ‚Ñ‹Ð¹, Ð½Ð¾ ÐºÐ¾Ð¼Ð¿Ð°ÐºÑ‚Ð½Ñ‹Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚ (3â€“7 Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¹), Ñ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ð¼Ð¸ Ð´ÐµÑ‚Ð°Ð»ÑÐ¼Ð¸.";
  } else if (answerMode === "long") {
    modeInstruction = "Ð ÐµÐ¶Ð¸Ð¼ long: Ð¼Ð¾Ð¶Ð½Ð¾ Ð¾Ñ‚Ð²ÐµÑ‡Ð°Ñ‚ÑŒ Ð¿Ð¾Ð´Ñ€Ð¾Ð±Ð½Ð¾, ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾, Ñ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð°Ð¼Ð¸ Ð¸ Ð¿Ð¾ÑÑÐ½ÐµÐ½Ð¸ÑÐ¼Ð¸.";
  }

  const currentUserName =
    [msg?.from?.first_name, msg?.from?.last_name].filter(Boolean).join(" ").trim() ||
    (msg?.from?.username ? `@${msg.from.username}` : "Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ");

  const systemPrompt = buildSystemPrompt(answerMode, modeInstruction, projectCtx || "", {
    isMonarch: monarchNow,
    currentUserName,
  });

  // ==========================================================
  // STAGE 8A â€” RECALL ENGINE (SKELETON)
  // - wiring only
  // - default disabled via RECALL_ENABLED
  // - fail-open (must not break production)
  // ==========================================================
  let recallCtx = "";
  try {
    const recall = getRecallEngine({ db: pool, logger: console });
    recallCtx = await recall.buildRecallContext({
      chatId: chatIdStr,
      globalUserId,
      query: effective,
      limit: 5,
    });
  } catch (e) {
    console.error("ERROR RecallEngine buildRecallContext failed (fail-open):", e);
  }

  //  DEBUG (STAGE 8A): prove recall is wired + non-empty
  try {
    console.log("RECALL DBG", {
      enabled: process.env.RECALL_ENABLED,
      hasRecallCtx: Boolean(recallCtx && recallCtx.trim()),
      recallLen: recallCtx ? recallCtx.length : 0,
      chatId: String(chatIdStr),
      senderId: String(senderIdStr),
      globalUserId: globalUserId ? String(globalUserId) : null,
      q: String(effective || "").slice(0, 60),
    });
  } catch (_) {}

  // ==========================================================
  // STAGE 8A GUARD â€” BLOCK AI IF RECALL TOO WEAK (anti-hallucination)
  // - If query is time-based (yesterday/N days ago/last week) AND recallCtx is too small,
  //   do NOT call AI at all. Return deterministic "no data" reply.
  // - This prevents GPT from inventing calendar dates (observed in prod).
  // - Fail-open: if guard crashes, continue normal flow.
  // ==========================================================
  try {
    const timeCtx = createTimeContext({ userTimezoneFromDb: null });
    const parsed = timeCtx.parseHumanDate(effective);

    if (parsed) {
      const recallLines = (recallCtx || "")
        .split("\n")
        .filter((l) => l.startsWith("U:") || l.startsWith("A:")).length;

      // 4 lines ~= 2 turns (U/A * 2). Below that GPT tends to "invent" dates/events.
      if (recallLines < 4) {
        try {
          await bot.sendMessage(chatId, "Ð’ Ð¿Ð°Ð¼ÑÑ‚Ð¸ Ð½ÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð·Ð° ÑÑ‚Ð¾Ñ‚ Ð¿ÐµÑ€Ð¸Ð¾Ð´.");
        } catch (e) {
          console.error("ERROR Guard send error:", e);
        }
        return; // ðŸš¨ STOP â€” do NOT call AI
      }
    }
  } catch (e) {
    console.error("ERROR STAGE 8A guard failed (fail-open):", e);
  }

  // ==========================================================
  // STAGE 8B â€” ALREADY-SEEN DETECTOR (SKELETON)
  // - wiring only
  // - fail-open
  // - no blocking logic in 8B
  //
  // STAGE 8C.1 â€” soft reaction flag (NO behavior change yet)
  // ==========================================================
  let softReaction = false;
  try {
    const alreadySeen = getAlreadySeenDetector({ db: pool, logger: console });

    const alreadySeenTriggered = await alreadySeen.check({
      chatId: chatIdStr,
      globalUserId,
      text: effective,
    });

    // STAGE 8C.1 â€” soft reaction flag (no behavior change yet)
    softReaction = Boolean(alreadySeenTriggered);
  } catch (e) {
    console.error("ERROR AlreadySeenDetector check failed (fail-open):", e);
  }

  // ==========================================================
  // STAGE 8C.2 â€” Soft Hint reply (UI-level, no blocking)
  // - only when softReaction === true
  // - no Recall changes
  // - no Memory changes
  // - fail-open
  // ==========================================================
  if (softReaction === true) {
    try {
      await bot.sendMessage(
        chatId,
        "ðŸ’¡ ÐŸÐ¾Ñ…Ð¾Ð¶Ðµ, Ð¼Ñ‹ ÑÑ‚Ð¾ ÑƒÐ¶Ðµ Ð¾Ð±ÑÑƒÐ¶Ð´Ð°Ð»Ð¸ Ð½ÐµÐ´Ð°Ð²Ð½Ð¾. Ð•ÑÐ»Ð¸ Ñ…Ð¾Ñ‡ÐµÑˆÑŒ â€” ÑƒÑ‚Ð¾Ñ‡Ð½Ð¸, Ñ‡Ñ‚Ð¾ Ð¸Ð¼ÐµÐ½Ð½Ð¾ Ð¿Ñ€Ð¾Ð´Ð¾Ð»Ð¶Ð¸Ñ‚ÑŒ Ð¸Ð»Ð¸ Ñ‡Ñ‚Ð¾ Ð¸Ð·Ð¼ÐµÐ½Ð¸Ð»Ð¾ÑÑŒ."
      );
    } catch (e) {
      console.error("ERROR Telegram send error (soft hint):", e);
      // fail-open
    }
  }

  //  FIX: role guard must use monarchNow (real identity), not bypass (router shortcut)
  const roleGuardPrompt = monarchNow
    ? "SYSTEM ROLE: Ñ‚ÐµÐºÑƒÑ‰Ð¸Ð¹ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ = MONARCH (Ñ€Ð°Ð·Ñ€ÐµÑˆÐµÐ½Ð¾ Ð¾Ð±Ñ€Ð°Ñ‰Ð°Ñ‚ÑŒÑÑ 'ÐœÐ¾Ð½Ð°Ñ€Ñ…', 'Ð“Ð°Ñ€Ð¸Ðº')."
    : "SYSTEM ROLE: Ñ‚ÐµÐºÑƒÑ‰Ð¸Ð¹ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ ÐÐ• Ð¼Ð¾Ð½Ð°Ñ€Ñ…. Ð—Ð°Ð¿Ñ€ÐµÑ‰ÐµÐ½Ð¾ Ð¾Ð±Ñ€Ð°Ñ‰Ð°Ñ‚ÑŒÑÑ 'ÐœÐ¾Ð½Ð°Ñ€Ñ…', 'Ð’Ð°ÑˆÐµ Ð’ÐµÐ»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾', 'Ð“Ð¾ÑÑƒÐ´Ð°Ñ€ÑŒ'. ÐÐ°Ð·Ñ‹Ð²Ð°Ð¹: 'Ð³Ð¾ÑÑ‚ÑŒ' Ð¸Ð»Ð¸ Ð½ÐµÐ¹Ñ‚Ñ€Ð°Ð»ÑŒÐ½Ð¾ (Ð²Ñ‹/Ñ‚Ñ‹).";

  const messages = [
    { role: "system", content: systemPrompt },
    recallCtx
      ? {
          role: "system",
          content:
            `RECALL CONTEXT (Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ ÐºÐ°Ðº Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ Ñ‡Ð°Ñ‚Ð°):\n${recallCtx}\n\n` +
            `ÐŸÐ ÐÐ’Ð˜Ð›Ðž: ÐµÑÐ»Ð¸ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ ÑÐ¿Ñ€Ð°ÑˆÐ¸Ð²Ð°ÐµÑ‚ "Ñ‡Ñ‚Ð¾ Ð¼Ñ‹ Ð¾Ð±ÑÑƒÐ¶Ð´Ð°Ð»Ð¸ Ð²Ñ‡ÐµÑ€Ð°/Ñ€Ð°Ð½ÑŒÑˆÐµ" â€” ` +
            `Ð¾Ñ‚Ð²ÐµÑ‡Ð°Ð¹, Ð¾Ð¿Ð¸Ñ€Ð°ÑÑÑŒ Ð½Ð° RECALL CONTEXT. ` +
            `ÐÐµ Ð³Ð¾Ð²Ð¾Ñ€Ð¸ "Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ñ Ð½Ðµ ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÑ‚ÑÑ". ` +
            `Ð•ÑÐ»Ð¸ Ñ‚Ð¾Ñ‡Ð½Ð¾Ð¹ Ð´Ð°Ñ‚Ñ‹/Ð²Ñ‡ÐµÑ€Ð° Ð½ÐµÑ‚ â€” ÑÐºÐ°Ð¶Ð¸ Ñ‡ÐµÑÑ‚Ð½Ð¾: "Ð²Ð¸Ð¶Ñƒ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ", Ð¸ Ð¿ÐµÑ€ÐµÑ‡Ð¸ÑÐ»Ð¸ Ð¸Ñ….`,
        }
      : null,
    { role: "system", content: roleGuardPrompt },
    ...history,
    { role: "user", content: effective },
  ];

  // Remove null entries (when recallCtx is empty)
  const filtered = messages.filter(Boolean);

  let maxTokens = 350;
  let temperature = 0.6;
  if (answerMode === "short") {
    maxTokens = 150;
    temperature = 0.3;
  } else if (answerMode === "long") {
    maxTokens = 900;
    temperature = 0.8;
  }

  // ---- OBSERVABILITY (minimal): log AI call with reason + cost level ----
  const aiReason = "chat.reply";
  const aiMetaBase = {
    handler: "chat",
    reason: aiReason,
    aiCostLevel: classification.aiCostLevel,
    answerMode,
    max_completion_tokens: maxTokens,
    temperature,
    chatId: chatIdStr,
    senderId: senderIdStr,
    messageId,
    globalUserId,
  };

  try {
    console.info("AI_CALL_START", aiMetaBase);
  } catch (_) {}

  try {
    await logInteraction(chatIdStr, { ...classification, event: "AI_CALL_START", ...aiMetaBase });
  } catch (e) {
    console.error("ERROR logInteraction (AI_CALL_START) error:", e);
  }

  const t0 = Date.now();
  // --------------------------------------------

  let aiReply = "";
  try {
    aiReply = await callAI(filtered, classification.aiCostLevel, {
      max_completion_tokens: maxTokens,
      temperature,
    });
  } catch (e) {
    console.error("ERROR AI error:", e);

    const msgText = e?.message ? String(e.message) : "unknown";
    aiReply = monarchNow ? `ERROR: ÐžÑˆÐ¸Ð±ÐºÐ° Ð²Ñ‹Ð·Ð¾Ð²Ð° Ð˜Ð˜: ${msgText}` : "ERROR: ÐžÑˆÐ¸Ð±ÐºÐ° Ð²Ñ‹Ð·Ð¾Ð²Ð° Ð˜Ð˜.";
  }

  // ---- OBSERVABILITY (minimal): log AI result ----
  const dtMs = Date.now() - t0;
  const aiMetaEnd = {
    ...aiMetaBase,
    dtMs,
    replyChars: typeof aiReply === "string" ? aiReply.length : 0,
    ok: !(typeof aiReply === "string" && aiReply.startsWith("ERROR: ÐžÑˆÐ¸Ð±ÐºÐ° Ð²Ñ‹Ð·Ð¾Ð²Ð° Ð˜Ð˜")),
  };

  try {
    console.info("AI_CALL_END", aiMetaEnd);
  } catch (_) {}

  try {
    await logInteraction(chatIdStr, { ...classification, event: "AI_CALL_END", ...aiMetaEnd });
  } catch (e) {
    console.error("ERROR logInteraction (AI_CALL_END) error:", e);
  }
  // --------------------------------------------

  // ==========================================================
  // STAGE 7B.4 + 7B.10 + 7B.2 â€” Log SG output to chat_messages (assistant)
  // - content stored redacted
  // - text_hash stored
  // - platform_message_id is unknown here (null)
  // fail-open (must not break production)
  // ==========================================================
  try {
    const transport = "telegram";
    const chatType = msg?.chat?.type || null;

    const assistantRedactedFull = redactText(typeof aiReply === "string" ? aiReply : "");
    const assistantTextHash = sha256Text(assistantRedactedFull);

    const assistantContentForDb =
      typeof assistantRedactedFull === "string" && assistantRedactedFull.length > MAX_CHAT_MESSAGE_CHARS
        ? assistantRedactedFull.slice(0, MAX_CHAT_MESSAGE_CHARS)
        : typeof assistantRedactedFull === "string"
          ? assistantRedactedFull
          : "";

    const assistantTruncatedForDb =
      typeof assistantRedactedFull === "string" && assistantRedactedFull.length > MAX_CHAT_MESSAGE_CHARS;

    const meta = {
      senderIdStr,
      chatIdStr,
      in_reply_to_message_id: messageId ?? null,
      globalUserId: globalUserId ?? null,
      handler: "chat",
      stage: "7B.4",
    };

    await pool.query(
      `
      INSERT INTO chat_messages (
        transport,
        chat_id,
        chat_type,
        global_user_id,
        sender_id,
        message_id,
        platform_message_id,
        text_hash,
        role,
        content,
        truncated,
        metadata,
        raw,
        schema_version
      )
      VALUES ($1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12::jsonb,$13::jsonb,$14)
      `,
      [
        transport,
        String(chatIdStr),
        chatType ? String(chatType) : null,
        globalUserId ? String(globalUserId) : null,
        null, // assistant has no sender_id (transport user id)
        null, // outgoing telegram message_id not available here
        null, // platform_message_id unknown for outgoing here
        assistantTextHash,
        "assistant",
        assistantContentForDb,
        Boolean(assistantTruncatedForDb),
        JSON.stringify(meta),
        JSON.stringify({}), // no raw for assistant
        1,
      ]
    );

    // STAGE 7B.8 â€” touch chat_meta (outbound assistant)
    try {
      await touchChatMeta({
        transport,
        chatId: String(chatIdStr),
        chatType: msg?.chat?.type || null,
        title: msg?.chat?.title || null,
        role: "assistant",
      });
    } catch (_) {}
  } catch (e) {
    console.error("ERROR STAGE 7B.4 chat_messages assistant insert failed (fail-open):", e);
  }

  //  STAGE 7.2: save pair with globalUserId
  try {
    await saveChatPair(chatIdStr, effective, aiReply, {
      globalUserId,
      transport: "telegram",
      metadata: { senderIdStr, chatIdStr, messageId },
      schemaVersion: 2,
    });
  } catch (e) {
    console.error("ERROR saveChatPair error:", e);
  }

  try {
    if (!monarchNow) aiReply = sanitizeNonMonarchReply(aiReply);
  } catch (e) {
    console.error("ERROR sanitizeNonMonarchReply error:", e);
  }

  try {
    await bot.sendMessage(chatId, aiReply);
  } catch (e) {
    console.error("ERROR Telegram send error:", e);
  }
}
