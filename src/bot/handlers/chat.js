// src/bot/handlers/chat.js
// extracted from messageRouter.js ‚Äî no logic changes (only safety-guards + token param fix + observability logs)
//
// STAGE 7.2 LOGIC: pass globalUserId to chat_memory (v2 columns)

export async function handleChatMessage({
  bot,
  msg,
  chatId,
  chatIdStr,
  senderIdStr,
  trimmed,
  bypass,
  MAX_HISTORY_MESSAGES,

  // ‚úÖ STAGE 7.2
  globalUserId = null,

  FileIntake,

  saveMessageToMemory,
  getChatHistory,
  saveChatPair,

  logInteraction,

  loadProjectContext,
  getAnswerMode,
  buildSystemPrompt,
  isMonarch,

  callAI,
  sanitizeNonMonarchReply,
}) {
  const messageId = msg.message_id ?? null;
  if (!trimmed) return;

  // ---- GUARDS (critical): never crash on wrong wiring ----
  const isMonarchFn = typeof isMonarch === "function" ? isMonarch : () => false;
  const monarchNow = isMonarchFn(senderIdStr);

  if (typeof callAI !== "function") {
    const details =
      "callAI is not a function (router wiring error: pass { callAI } into handleChatMessage).";
    const text = monarchNow ? `‚ö†Ô∏è –û—à–∏–±–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: ${details}` : "‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ –ò–ò.";

    try {
      await bot.sendMessage(chatId, text);
    } catch (e) {
      console.error("‚ùå Telegram send error (callAI guard):", e);
    }
    return;
  }
  // --------------------------------------------

  const summarizeMediaAttachment =
    typeof FileIntake?.summarizeMediaAttachment === "function"
      ? FileIntake.summarizeMediaAttachment
      : () => null;

  const mediaSummary = summarizeMediaAttachment(msg);

  const decisionFn =
    typeof FileIntake?.buildEffectiveUserTextAndDecision === "function"
      ? FileIntake.buildEffectiveUserTextAndDecision
      : null;

  const decision = decisionFn
    ? decisionFn(trimmed, mediaSummary)
    : {
        effectiveUserText: trimmed,
        shouldCallAI: Boolean(trimmed),
        directReplyText: Boolean(trimmed) ? null : "–ù–∞–ø–∏—à–∏ —Ç–µ–∫—Å—Ç–æ–º, —á—Ç–æ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å.",
      };

  const effective = (decision?.effectiveUserText || "").trim();
  const shouldCallAI = Boolean(decision?.shouldCallAI);
  const directReplyText = decision?.directReplyText || null;

  if (directReplyText) {
    try {
      await bot.sendMessage(chatId, directReplyText);
    } catch (e) {
      console.error("‚ùå Telegram send error (directReplyText):", e);
    }
    return;
  }

  if (!shouldCallAI) {
    try {
      await bot.sendMessage(chatId, "–ù–∞–ø–∏—à–∏ —Ç–µ–∫—Å—Ç–æ–º, —á—Ç–æ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å.");
    } catch (e) {
      console.error("‚ùå Telegram send error (shouldCallAI):", e);
    }
    return;
  }

  // ‚úÖ STAGE 7.2: save with globalUserId + metadata
  try {
    await saveMessageToMemory(chatIdStr, "user", effective, {
      globalUserId,
      transport: "telegram",
      metadata: { senderIdStr, chatIdStr, messageId },
      schemaVersion: 1,
    });
  } catch (e) {
    console.error("‚ùå saveMessageToMemory error:", e);
  }

  let history = [];
  try {
    // ‚úÖ STAGE 7.2: load history filtered by globalUserId (if provided)
    history = await getChatHistory(chatIdStr, MAX_HISTORY_MESSAGES, { globalUserId });
  } catch (e) {
    console.error("‚ùå getChatHistory error:", e);
  }

  const classification = { taskType: "chat", aiCostLevel: "low" };
  try {
    await logInteraction(chatIdStr, classification);
  } catch (e) {
    console.error("‚ùå logInteraction error:", e);
  }

  let projectCtx = "";
  try {
    projectCtx = await loadProjectContext();
  } catch (e) {
    console.error("‚ùå loadProjectContext error:", e);
  }

  const answerMode = getAnswerMode(chatIdStr);

  let modeInstruction = "";
  if (answerMode === "short") {
    modeInstruction =
      "–†–µ–∂–∏–º short: –æ—Ç–≤–µ—á–∞–π –æ—á–µ–Ω—å –∫—Ä–∞—Ç–∫–æ (1‚Äì2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è), —Ç–æ–ª—å–∫–æ –ø–æ —Å—É—â–µ—Å—Ç–≤—É, –±–µ–∑ –ª–∏—à–Ω–∏—Ö –¥–µ—Ç–∞–ª–µ–π.";
  } else if (answerMode === "normal") {
    modeInstruction =
      "–†–µ–∂–∏–º normal: –¥–∞–≤–∞–π —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç—ã–π, –Ω–æ –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç (3‚Äì7 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π), —Å –∫–ª—é—á–µ–≤—ã–º–∏ –¥–µ—Ç–∞–ª—è–º–∏.";
  } else if (answerMode === "long") {
    modeInstruction =
      "–†–µ–∂–∏–º long: –º–æ–∂–Ω–æ –æ—Ç–≤–µ—á–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω–æ, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ, —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∏ –ø–æ—è—Å–Ω–µ–Ω–∏—è–º–∏.";
  }

  const currentUserName =
    [msg?.from?.first_name, msg?.from?.last_name].filter(Boolean).join(" ").trim() ||
    (msg?.from?.username ? `@${msg.from.username}` : "–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å");

  const systemPrompt = buildSystemPrompt(answerMode, modeInstruction, projectCtx || "", {
    isMonarch: monarchNow,
    currentUserName,
  });

  // ‚úÖ FIX: role guard must use monarchNow (real identity), not bypass (router shortcut)
  const roleGuardPrompt = monarchNow
    ? "SYSTEM ROLE: —Ç–µ–∫—É—â–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å = MONARCH (—Ä–∞–∑—Ä–µ—à–µ–Ω–æ –æ–±—Ä–∞—â–∞—Ç—å—Å—è '–ú–æ–Ω–∞—Ä—Ö', '–ì–∞—Ä–∏–∫')."
    : "SYSTEM ROLE: —Ç–µ–∫—É—â–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ù–ï –º–æ–Ω–∞—Ä—Ö. –ó–∞–ø—Ä–µ—â–µ–Ω–æ –æ–±—Ä–∞—â–∞—Ç—å—Å—è '–ú–æ–Ω–∞—Ä—Ö', '–í–∞—à–µ –í–µ–ª–∏—á–µ—Å—Ç–≤–æ', '–ì–æ—Å—É–¥–∞—Ä—å'. –ù–∞–∑—ã–≤–∞–π: '–≥–æ—Å—Ç—å' –∏–ª–∏ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ (–≤—ã/—Ç—ã).";

  const messages = [
    { role: "system", content: systemPrompt },
    { role: "system", content: roleGuardPrompt },
    ...history,
    { role: "user", content: effective },
  ];

  let maxTokens = 350;
  let temperature = 0.6;
  if (answerMode === "short") {
    maxTokens = 150;
    temperature = 0.3;
  } else if (answerMode === "long") {
    maxTokens = 900;
    temperature = 0.8;
  }

  // ---- OBSERVABILITY (minimal): log AI call with reason + cost level ----
  const aiReason = "chat.reply";
  const aiMetaBase = {
    handler: "chat",
    reason: aiReason,
    aiCostLevel: classification.aiCostLevel,
    answerMode,
    max_completion_tokens: maxTokens,
    temperature,
    chatId: chatIdStr,
    senderId: senderIdStr,
    messageId,
    globalUserId,
  };

  try {
    console.info("üßæ AI_CALL_START", aiMetaBase);
  } catch (_) {}

  try {
    await logInteraction(chatIdStr, { ...classification, event: "AI_CALL_START", ...aiMetaBase });
  } catch (e) {
    console.error("‚ùå logInteraction (AI_CALL_START) error:", e);
  }

  const t0 = Date.now();
  // --------------------------------------------

  let aiReply = "";
  try {
    aiReply = await callAI(messages, classification.aiCostLevel, {
      max_completion_tokens: maxTokens,
      temperature,
    });
  } catch (e) {
    console.error("‚ùå AI error:", e);

    const msgText = e?.message ? String(e.message) : "unknown";
    aiReply = monarchNow ? `‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ –ò–ò: ${msgText}` : "‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ –ò–ò.";
  }

  // ---- OBSERVABILITY (minimal): log AI result ----
  const dtMs = Date.now() - t0;
  const aiMetaEnd = {
    ...aiMetaBase,
    dtMs,
    replyChars: typeof aiReply === "string" ? aiReply.length : 0,
    ok: !(typeof aiReply === "string" && aiReply.startsWith("‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ –ò–ò")),
  };

  try {
    console.info("üßæ AI_CALL_END", aiMetaEnd);
  } catch (_) {}

  try {
    await logInteraction(chatIdStr, { ...classification, event: "AI_CALL_END", ...aiMetaEnd });
  } catch (e) {
    console.error("‚ùå logInteraction (AI_CALL_END) error:", e);
  }
  // --------------------------------------------

  // ‚úÖ STAGE 7.2: save pair with globalUserId
  try {
    await saveChatPair(chatIdStr, effective, aiReply, {
      globalUserId,
      transport: "telegram",
      metadata: { senderIdStr, chatIdStr, messageId },
      schemaVersion: 1,
    });
  } catch (e) {
    console.error("‚ùå saveChatPair error:", e);
  }

  try {
    if (!monarchNow) aiReply = sanitizeNonMonarchReply(aiReply);
  } catch (e) {
    console.error("‚ùå sanitizeNonMonarchReply error:", e);
  }

  try {
    await bot.sendMessage(chatId, aiReply);
  } catch (e) {
    console.error("‚ùå Telegram send error:", e);
  }
}
